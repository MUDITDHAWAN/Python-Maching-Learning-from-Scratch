# -*- coding: utf-8 -*-
"""ques4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17ggMp3Txe_YAtYMJ64C7066LJC2cTsx6
"""

# from google.colab import drive
# drive.mount('/content/drive')

import os
# os.chdir("/content/drive/My Drive/ML-Sem5/ML_Assignment-2/Dataset/")
os.chdir("./Dataset/")
import pandas as pd
import h5py
import numpy as np
import tqdm
import matplotlib.pyplot as plt
from math import sqrt
from math import pi
from math import exp
from math import log
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, accuracy_score
from sklearn.naive_bayes import GaussianNB

def read_data(file_name):
    """
    Reading the file and preprocessing the input and output.
    Encoded the one-hot output values to labels 

    Parameters
    ----------

    file_name : string - address of the data file (.h5) to be read 
    
    Returns
    -------
    X : 2-dimensional numpy array of shape (n_samples, n_features)
    y : 1-dimensional numpy array of shape (n_samples,)
    """

    ## read the .h5 file 
    data = h5py.File(file_name, 'r')
    X = data['X'].value
    Y = data['Y'].value

    ## convert the one hot encoding to labels 
    enc_Y = np.where(Y==1)[1]

    ## Shuffle the dataset 
    X, y = shuffle(X, enc_Y, random_state=0)

    return X, y

class GaussianNaiveBayes():
    """
    My implementation of Gaussian Naive Bayes.
    """
    def __init__(self):
        self.classes = []
        self.class_count = {}

    def stats_columns(self, X):
        """
        Takes the training data to calculate the statistics (mean and variance) required for each  attribute(/ column) 

        Parameters
        ----------

        X : 2-dimensional array of size (nb_samples, nb_features)
        
        Returns
        -------
        stats : a list containg mean and varaince corresponding to each column
        """
        stats = []

        ## loop over the columns
        for idx in range(X.shape[1]):
            curr_col = X[:, idx]

            ## for the column calculate mean 
            curr_mean = np.mean(curr_col, axis=0)

            ## for the column calculate variance   ( add 1e-9 to make it non-zero)
            curr_var = (np.std(curr_col, axis=0))**2 + 1e-09

            ## create a dictionary for each column and append in the final list 
            stats.append({'mean': curr_mean, 'var': curr_var})
        
        return stats

    def calculate_probability(self, x, mean, var):
        """
        Calculates Gaussian probability distribution function for x ( 1 sample )

        Parameters
        ----------

        x : 1-dimensional array of size (nb_features, ) corresponding to one sample

        mean : calculated mean for the distribution

        variance : calculated variance for the distribution
        
        Returns
        -------
        prob : the corresponding gaussian probability for the sample 
        """
        exponent = exp(-((x-mean)**2 / (2 * var )))

        return (1 / (sqrt(2 * pi* var))) * exponent


    def calc_prob_for_sample(self, sample):
        """
        Calculates Gaussian probability distribution function for x ( 1 sample ) for all the classes 

        Parameters
        ----------

        sample : 1-dimensional array of size (nb_features, ) corresponding to one sample
        
        Returns
        -------
        prob_for_class : dictionary containing the gaussian probability for the sample for tall the classes 
        """

        ## dict to save label wise probability 
        prob_for_class = {}

        ## loop over all labels 
        for label in self.classes:

            ## use the prior probability available for the class (using log of the probability for numeric stability )
            cond_prob_sample = log(self.class_probs[label])

            ## calculate the conditional probabilty for each attribute in the sample
            for col in range(self.nb_columns):

                ## use the previously calculated mean and variance 
                mean_for_sample = self.label_stats[label][col]['mean']
                var_for_sample = self.label_stats[label][col]['var']
                # print(sample[col])

                ## calculate the final conditional probability for the label 
                prob = self.calculate_probability(sample[col], mean_for_sample, var_for_sample)

                ## if the probability comes out to be zero add a small epsilon for numeric stability 
                if prob == 0.0:
                    prob+= 0.0000001
                
                ## add in the log condition probability
                cond_prob_sample += log(prob)
            
            ## save the log probability for the corresponding output class 
            prob_for_class[label] = cond_prob_sample 

        return prob_for_class

    def fit(self, X,y):
        """
        Calculates the probaility for each ouput label and conditional probability 
        for each attribute correspoinding to the labels 

        Parameters
        ----------

        X : 2-dimensional array of size (nb_samples, nb_features) 
        y : 1-dimensional array of size (nb_samples, ) 

        """

        ## save the output labels 
        self.classes = np.unique(y).tolist()

        ## save the number of features 
        self.nb_columns = X.shape[1]

        ## number of training samples 
        nb_samples = X.shape[0]

        ## empty dictionaries to save the log probabilities and stats 
        class_probs = {}
        label_stats = {}
        # print(self.classes)

        ## loop over the classes
        for label in self.classes:
            print(label)

            ## find indices with the label value 
            label_idx = np.where(y == label)
            # print(len(list(label_idx)[0]))

            ## segregate the data of the label
            label_X = X[label_idx]
            # print(label_X.shape[0], nb_samples)

            ## calculate the probability of that label
            class_probs[label] = label_X.shape[0] / nb_samples

            ## calculate column-wise stats foor the label
            label_stats[label] = self.stats_columns(label_X) 

            # print(label_stats)
            # break
        
        ## save them for prediction
        self.label_stats = label_stats
        self.class_probs = class_probs
    
    def predict(self, X):
        """
        Predicts the albel corresponding to the given data

        Parameters
        ----------

        X : 2-dimensional array of size (nb_samples, nb_features)  
        
        Returns
        -------
        y_pred : 1-dimensional array of size (nb_samples, )

        """
        y_pred = []

        ## loop over all the data samples 
        for idx in range(X.shape[0]):

            ## select one data point
            x = X[idx, :]

            ## calculate probability for all the labels
            dict_probs = self.calc_prob_for_sample(x)
            # print(dict_probs)

            ## take the label which has the maximum probability for that sample 
            pred_label_sample = max(dict_probs, key=dict_probs.get)
            # print(pred_label_sample)

            ## add it to the prediction list 
            y_pred.append(pred_label_sample)
        
        return np.asarray(y_pred)

def GNB(X_train, y_train, X_test, y_test):

    ## Create an instance of the model 
    g = GaussianNaiveBayes()

    ## train the model 
    g.fit(X_train, y_train)

    ## make prediction on the test set 
    y_pred = g.predict(X_test)

    ## calculate the accuracy on the test set
    acc_score = accuracy_score(y_test, y_pred)

    return acc_score

def sklearn_GNB(X_train, y_train, X_test, y_test):

    ## Create an instance of the model 
    clf = GaussianNB()
    
    ## train the model
    clf.fit(X_train, y_train)

    ## calculate the accuracy on the test set
    acc_score = clf.score(X_test, y_test)

    return acc_score

def q4(file_name):

    ## read the data file 
    X, y = read_data(file_name)

    ## shuffle and spli thte dataset 
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=8)

    ## Accuracy on scratch implemetation
    print("Scratch Implementation Accuracy : ", 100*(GNB(X_train, y_train, X_test, y_test)))

    ## Accuracy on sklearn implemetation
    print("Sklearn Implementation Accuracy : ", 100*(sklearn_GNB(X_train, y_train, X_test, y_test)))

q4("part_A_train.h5")

q4("part_B_train.h5")

