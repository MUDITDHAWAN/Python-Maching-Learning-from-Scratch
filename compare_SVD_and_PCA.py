# -*- coding: utf-8 -*-
"""ques1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15dQtPoA-dfACJs9Akhn9_DuSgWZF5soa
"""

# from google.colab import drive
# drive.mount('/content/drive')

import os
# os.chdir("/content/drive/My Drive/ML-Sem5/ML_Assignment-2/Dataset/")
os.chdir("./Dataset/")
import pandas as pd
import h5py
import numpy as np
import matplotlib.pyplot as plt
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split, StratifiedShuffleSplit
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.multiclass import OneVsRestClassifier
from sklearn.decomposition import PCA
from sklearn.decomposition import TruncatedSVD
from sklearn.manifold import TSNE
import seaborn as sns

plot_dir = "../Plots/"

## Read data and split into X, Y arrays
q1_data = h5py.File('part_A_train.h5', 'r')
X = q1_data['X'].value
Y = q1_data['Y'].value

## convert one-hot encoding to class labels 
enc_Y = np.where(Y==1)[1]

## Shuffling the dataset
X, y = shuffle(X, enc_Y, random_state=0)

## Performing Stratified Splitting 

# Creating an instance of the sampler 
sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=10)

# Finding the class-wise indices split into train and test for n_splits
sss.get_n_splits(X, y)

## Looping on the n_splits to get indices
for train_index, test_index in sss.split(X, y):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]

"""# Class Frequency after Stratified Sampling"""

## create df (simply use Groupby object to plot class frequency)

## Entire Data 
df = pd.DataFrame(data=np.concatenate((X,y.reshape((y.shape[0], 1))), axis=1))

## Training data 
df_train = pd.DataFrame(data=np.concatenate((X_train,y_train.reshape((y_train.shape[0], 1))), axis=1))

## Testing data 
df_test = pd.DataFrame(data=np.concatenate((X_test,y_test.reshape((y_test.shape[0], 1))), axis=1))

## Plotting the Label-wise count for all the data frames 
df_train.groupby(784).count()[0].plot(legend=True) ## legend = 0
df_test.groupby(784).count()[1].plot(legend=True) ## legend = 1
df.groupby(784).count()[2].plot(legend=True) ## legend = 2
plt.savefig(plot_dir+"Q1_B.png")
plt.xlabel("Class")
plt.ylabel("Number of samples")
plt.title("Class frequency")
plt.show()

## Plotting the Label-wise percentage for all the data frames 
df_train.groupby(784).count()[0].plot(kind="pie", legend=True, autopct='%1.0f%%') ## legend = 0
plt.title("Training Dataset")
plt.show()
df_test.groupby(784).count()[1].plot(kind="pie", legend=True, autopct='%1.0f%%') ## legend = 1
plt.title("Testing Dataset Dataset")
plt.show()
df.groupby(784).count()[2].plot(kind="pie", legend=True, autopct='%1.0f%%') ## legend = 2
plt.title("Entire Dataset")
plt.show()


"""## PCA"""

## Normalize the data to be put into PCA 
scaler = StandardScaler()
# Fit only on the train data, and then the saved parameter used transform the test data as well
scaler.fit(X_train)

X_train_norm = scaler.transform(X_train)
X_test_norm = scaler.transform(X_test)

# Create an instance of the PCA model based on variance retained 
pca = PCA(n_components=300, random_state=42)

## Fit the PCA model on the training set
pca.fit(X_train_norm)

## Transform the the training and testing set based on the saved parameters  
X_train_pca = pca.transform(X_train_norm)
X_test_pca = pca.transform(X_test_norm)

## Create an instance of the logistic regression model with OneVsRest Classifier for the multi-label problem 
logisticRegr = OneVsRestClassifier(LogisticRegression(max_iter=1000))

## Train the Logisitc regresiion model on training transformed data 
logisticRegr.fit(X_train_pca, y_train)

## Calculate the score on Testing Data 
print("Accuracy on Test set (Using PCA) : ", logisticRegr.score(X_test_pca, y_test))

"""## SVD"""

## Create an instance of the SVD Model 
svd = TruncatedSVD(n_components=300, random_state=42)

## Fit only on the training data 
svd.fit(X_train)

## Transform the training and test data
X_train_svd = svd.transform(X_train_norm)
X_test_svd = svd.transform(X_test_norm)

## Create an instance of the logistic regression model with OneVsRest Classifier for the multi-label problem 
logisticRegr = OneVsRestClassifier(LogisticRegression(max_iter=1000))

## Train the Logisitc regresiion model on training transformed data 
logisticRegr.fit(X_train_svd, y_train)

print("Accuracy on Test set (Using SVD) : ", logisticRegr.score(X_test_svd, y_test))

"""# t-SNE visualizations

## PCA
"""

## Fit t-SNE on the PCA transformed data 
X_embedded_pca = TSNE(n_components=2).fit_transform(X_train_pca)

## Plotting the t-SNE visualization 
plt.figure(figsize=(16,10))
sns.scatterplot(
    x=X_embedded_pca[:,1], y=X_embedded_pca[:,0],
    hue=y_train,
    palette=sns.color_palette(),
    legend="full",
    alpha=1
)
plt.title("t-SNE after PCA")
plt.savefig(plot_dir+"Q1_E.png")
plt.show()

"""## SVD"""

## Fit t-SNE on the SVD transformed data 
X_embedded_svd = TSNE(n_components=2).fit_transform(X_train_svd)

## Plotting the t-SNE visualization 
plt.figure(figsize=(16,10))
sns.scatterplot(
    x=X_embedded_svd[:,0], y=X_embedded_svd[:,1],
    hue=y_train,
    palette=sns.color_palette(),
    legend="full",
    alpha=1
)
plt.title("t-SNE after SVD")
plt.savefig(plot_dir+"Q1_F.png")
plt.show()
